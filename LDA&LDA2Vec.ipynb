{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sciense Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1. Target formulation\n",
    "\n",
    "###2. Text cleaning\n",
    "\n",
    "###3. Model building\n",
    "\n",
    "###4. Results using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"classify.png\", style=\"max-width:90%; width: 80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LDA.png\", style=\"max-width:90%; width: 80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"english\")\n",
    "from pattern.en import lemma\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "corpus_savepath = 'gensimldatest.mm'\n",
    "\n",
    "\n",
    "#basic preprocessing and lemmatization \n",
    "\n",
    "texts = fetch_20newsgroups(subset='train').data\n",
    "texts = [unicode(d.lower()) for d in texts]\n",
    "texts = [\"\".join((char if char.isalpha() else \" \") for char in text).split() for text in texts]\n",
    "texts = [[word for word in text[:1000] if word not in stopwords] for text in texts]\n",
    "\n",
    "#creating frequency dictionary for tokens in text\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "#removing very infrequent and very frequent tokens in corpus\n",
    "texts = [[token for token in text if (frequency[token] > 10 and len(token) > 2 and frequency[token] < len(texts)*0.2)] for text in texts]\n",
    "\n",
    "#creating an LDA model\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "gensim.corpora.MmCorpus.serialize(corpus_savepath, corpus)\n",
    "modelled_corpus = gensim.corpora.MmCorpus(corpus_savepath)\n",
    "lda1 = gensim.models.ldamodel.LdaModel(modelled_corpus, num_topics=10, update_every=100, passes=20, id2word=dictionary, alpha='auto', eval_every=5)\n",
    "\n",
    "#returning the resulting topics\n",
    "lda1.show_topics(num_topics=10, num_words=30, formatted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 0.53758295629888098),\n",
       " (2, 0.18588873448018561),\n",
       " (8, 0.15471837618089992),\n",
       " (1, 0.10088456902754719)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = \"Indianes women hockey could never recover from three early setbacks and ended up losing the Hawkes \\\n",
    "Bay Cup quarterfinal 1-3 to Japan, today. \"\n",
    "\n",
    "texts = [unicode(d.lower()) for d in new.split()]\n",
    "text1 = dictionary.doc2bow(texts)\n",
    "sorted(lda[text1], key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 0.48670069241050118),\n",
       " (0, 0.24817498942542299),\n",
       " (2, 0.13441145401025376),\n",
       " (5, 0.10996778497322443)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = \" The bible is not the WORD OF GOD. And if we believe it as such, then we have made the bible into an idol. \\\n",
    "The Bible makes the claim \\\n",
    "of being the inspired word of God, God's message to humanity.\"\n",
    "\n",
    "texts = [unicode(d.lower()) for d in new.split()]\n",
    "text1 = dictionary.doc2bow(texts)\n",
    "sorted(lda[text1], key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 0.71185683311735759),\n",
       " (4, 0.21705128852495115),\n",
       " (9, 0.013788021255459389),\n",
       " (1, 0.011798482860266721)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = \"Having had windows live mail working satisfactorily for some time \"\n",
    "\n",
    "texts = [unicode(d.lower()) for d in new.split()]\n",
    "text1 = dictionary.doc2bow(texts)\n",
    "lda[text1]\n",
    "sorted(lda[text1], key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 0.77765272176558953),\n",
       " (1, 0.03776186665686139),\n",
       " (8, 0.031314062631686457),\n",
       " (0, 0.029998017363570186),\n",
       " (5, 0.025090172634431366),\n",
       " (7, 0.022590981930248007),\n",
       " (6, 0.022449774249397886),\n",
       " (3, 0.020767332524668225),\n",
       " (4, 0.016492515697128261),\n",
       " (2, 0.015882554546418556)]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = dictionary.doc2bow([\"government\"])\n",
    "lda[text1]\n",
    "sorted(lda[text1], key = lambda x: x[1], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"lda2.jpg\", style=\"max-width:90%; width: 60%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lda2vec import preprocess, LDA2Vec, Corpus\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from chainer import serializers\n",
    "from chainer import cuda\n",
    "import numpy as np\n",
    "import os.path\n",
    "import logging\n",
    "\n",
    "\n",
    "# Optional: moving the model to the GPU makes it ~10x faster\n",
    "# set to False if you're having problems with Chainer and CUDA\n",
    "gpu = cuda.available\n",
    "\n",
    "logging.basicConfig()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "corpus_savepath = 'gensimldatest.mm'\n",
    "\n",
    "#basic preprocessing and lemmatization \n",
    "texts = fetch_20newsgroups(subset='train').data\n",
    "texts = [unicode(d.lower()) for d in texts]\n",
    "texts = [\"\".join((char if char.isalpha() else \" \") for char in text).split() for text in texts]\n",
    "texts = [[word for word in text[:2000] if word not in cachedStopWords] for text in texts]\n",
    "\n",
    "texts = [unicode(d) for d in texts]\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess data\n",
    "max_length = 1000   # Limit of 1k words per document\n",
    "tokens, vocab = preprocess.tokenize(texts, max_length, tag=False,\n",
    "                                    parse=False, entity=False)\n",
    "corpus = Corpus()\n",
    "# Make a ranked list of rare vs frequent words\n",
    "corpus.update_word_count(tokens)\n",
    "corpus.finalize()\n",
    "# The tokenization uses spaCy indices, and so may have gaps\n",
    "# between indices for words that aren't present in our dataset.\n",
    "# This builds a new compact index\n",
    "compact = corpus.to_compact(tokens)\n",
    "# Remove extremely rare words\n",
    "pruned = corpus.filter_count(compact, min_count=50)\n",
    "# Words tend to have power law frequency, so selectively\n",
    "# downsample the most prevalent words\n",
    "clean = corpus.subsample_frequent(pruned)\n",
    "# Now flatten a 2D array of document per row and word position\n",
    "# per column to a 1D array of words. This will also remove skips\n",
    "# and OoV words\n",
    "doc_ids = np.arange(pruned.shape[0])\n",
    "flattened, (doc_ids,) = corpus.compact_to_flat(pruned, doc_ids)\n",
    "\n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "# Number of documents\n",
    "n_docs = len(texts)\n",
    "# Number of unique words in the vocabulary\n",
    "n_words = flattened.max() + 1\n",
    "# Number of dimensions in a single word vector\n",
    "n_hidden = 128\n",
    "# Number of topics to fit\n",
    "n_topics = 20\n",
    "# Get the count for each key\n",
    "counts = corpus.keys_counts[:n_words]\n",
    "# Get the string representation for every compact key\n",
    "words = corpus.word_list(vocab)[:n_words]\n",
    "\n",
    "# Fit the model\n",
    "model = LDA2Vec(n_words, n_hidden, counts, dropout_ratio=0.2)\n",
    "model.add_categorical_feature(n_docs, n_topics, name='document_id')\n",
    "model.finalize()\n",
    "\n",
    "for _ in range(50):\n",
    "    model.top_words_per_topic('document_id', words)\n",
    "    if gpu:\n",
    "        model.to_gpu()\n",
    "    model.fit(flattened, categorical_features=[doc_ids], fraction=1e-3,\n",
    "              epochs=1)\n",
    "    model.to_cpu()\n",
    "serializers.save_hdf5('model_film_20news.hdf5', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "# Number of documents\n",
    "n_docs = len(texts)\n",
    "# Number of unique words in the vocabulary\n",
    "n_words = flattened.max() + 1\n",
    "# Number of dimensions in a single word vector\n",
    "n_hidden = 128\n",
    "# Number of topics to fit\n",
    "n_topics = 20\n",
    "# Get the count for each key\n",
    "counts = corpus.keys_counts[:n_words]\n",
    "# Get the string representation for every compact key\n",
    "words = corpus.word_list(vocab)[:n_words]\n",
    "\n",
    "# Fit the model\n",
    "model = LDA2Vec(n_words, n_hidden, counts, dropout_ratio=0.2)\n",
    "model.add_categorical_feature(n_docs, n_topics, name='document_id')\n",
    "model.finalize()\n",
    "\n",
    "for _ in range(50):\n",
    "    model.top_words_per_topic('document_id', words)\n",
    "    if gpu:\n",
    "        model.to_gpu()\n",
    "    model.fit(flattened, categorical_features=[doc_ids], fraction=1e-3,\n",
    "              epochs=1)\n",
    "    model.to_cpu()\n",
    "serializers.save_hdf5('model_film_20news.hdf5', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
